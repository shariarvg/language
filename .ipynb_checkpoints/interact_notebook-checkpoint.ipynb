{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d741042b-6036-416d-89cb-2b378bff2f35",
   "metadata": {},
   "outputs": [],
   "source": [
    "import basic_interact"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "dd187828-0f62-477f-96bf-b009b1549089",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Assistant created, with ID: asst_hwAATAQBmJMU6Xd8rIXfLjHB\n"
     ]
    }
   ],
   "source": [
    "import assistant_create"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c95b0abe-ae2d-4a68-b3d6-4ac168226eb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "conversation_history = []\n",
    "user_input = \"Hola, me llamo Josh. Me gusta las bibliotecas.\"\n",
    "scratchpad = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7c89f915-b7fd-4cb1-8f9b-faa73af15650",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'basic_interact' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_1865452/530313579.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mconvo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbasic_interact\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mConversation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'basic_interact' is not defined"
     ]
    }
   ],
   "source": [
    "convo = basic_interact.Conversation()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "b4e5abfa-9f35-42a0-b851-37fe793b7be1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "import time\n",
    "import json\n",
    "import re\n",
    "\n",
    "PROMPT = '''\n",
    "You are a native Spanish speaker helping a user practice Spanish conversation. Always reply to the user in Spanish.\n",
    "\n",
    "In addition, you are tracking the user's grammar and spelling mistakes. For each user input, write down any grammar or spelling errors you find in a scratchpad, in English. Store them without telling the user unless asked.\n",
    "\n",
    "Reply to every user prompt in a JSON-decodeable dictionary format, with keys\n",
    "- `assistant`: your Spanish response\n",
    "- `scratchpad_update`: any grammar or spelling issues detected (if any)\n",
    "\n",
    "When quoting titles or using quotation marks in your response, always escape double quotes (\\\") so the output is valid JSON.\n",
    "\n",
    "e.g. input is 'A m√≠ me gusta las bibliotecas' -> output: {\"assistant\": \"A m√≠ me gustan las bibliotecas tambi√©n. ¬øCu√°l es tu libro favorito?'\", \"scratchpad_update\":\"Incorrectly uses \\'me gusta\\' when subject (bibliotecas) is plural.\"}\n",
    "\n",
    "'''\n",
    "\n",
    "PROMPT_SENTINEL = '''\n",
    "You are a native Spanish speaker helping a user practice Spanish conversation. Always reply to the user in Spanish.\n",
    "\n",
    "Also, you are tracking the user's grammar and spelling mistakes in a scratchpad. After each user input, silently write any grammar or spelling mistakes you detect ‚Äî in English ‚Äî to this scratchpad.\n",
    "\n",
    "Respond in the following format:\n",
    "\n",
    "[Your full Spanish response to the user goes here ‚Äî no JSON, no quotes.]\n",
    "\n",
    "@@@END_OF_ASSISTANT@@@\n",
    "\n",
    "scratchpad_update: [Any grammar or spelling issues here, in English.]\n",
    "\n",
    "Only include the scratchpad_update line after the marker. Do not include the marker in the Spanish reply. Do not say anything else outside this format.\n",
    "'''\n",
    "\n",
    "\n",
    "\n",
    "with open(\"../../key.txt\", 'r') as f:\n",
    "    api_key = f.read().strip()\n",
    "\n",
    "openai.api_key = api_key\n",
    "client = openai.OpenAI(api_key=api_key)\n",
    "\n",
    "class Conversation():\n",
    "    def __init__(self):   \n",
    "        #self.assistant = client.beta.assistants.retrieve(\"asst_hwAATAQBmJMU6Xd8rIXfLjHB\")\n",
    "        #self.thread = client.beta.threads.create()\n",
    "        self.scratchpad = []\n",
    "        self.conversation_history = [] \n",
    "        \n",
    "    def extract_first_json_block(self, text):\n",
    "        match = re.search(r'\\{.*\\}', text, re.DOTALL)\n",
    "        if match:\n",
    "            json_str = match.group(0)\n",
    "            return json.loads(json_str)\n",
    "        else:\n",
    "            print(\"Raw reply_content:\\n\", repr(text))\n",
    "            raise ValueError(\"No JSON object found in response.\")\n",
    "\n",
    "    def query_gpt4(self, user_input):\n",
    "        system_prompt = PROMPT  # From above\n",
    "        messages = [{\"role\": \"system\", \"content\": system_prompt}] + self.conversation_history\n",
    "        messages.append({\"role\": \"user\", \"content\": user_input})\n",
    "        \n",
    "        response = client.chat.completions.create(\n",
    "            model=\"gpt-4\",\n",
    "            messages=messages,\n",
    "            temperature=0.7\n",
    "        )\n",
    "\n",
    "        # Parse response\n",
    "        data = self.extract_first_json_block(response.choices[0].message.content)\n",
    "        #data = json.loads(reply_content)\n",
    "        \n",
    "\n",
    "        # Update conversation and scratchpad\n",
    "        convo.conversation_history.append({\"role\": \"user\", \"content\": user_input})\n",
    "        convo.conversation_history.append({\"role\": \"assistant\", \"content\": response.choices[0].message.content})\n",
    "\n",
    "        if data.get(\"scratchpad_update\"):\n",
    "            self.scratchpad.append(data['scratchpad_update'])\n",
    "\n",
    "        print(data['assistant'])\n",
    "        \n",
    "    def query_gpt4_streaming(self, user_input):\n",
    "        system_prompt = PROMPT_SENTINEL\n",
    "        messages = [{\"role\": \"system\", \"content\": system_prompt}] + self.conversation_history\n",
    "        messages.append({\"role\": \"user\", \"content\": user_input})\n",
    "\n",
    "        stream = client.chat.completions.create(\n",
    "            model=\"gpt-4\",\n",
    "            messages=messages,\n",
    "            temperature=0.7,\n",
    "            stream=True\n",
    "        )\n",
    "\n",
    "        buffer = \"\"\n",
    "        assistant_text = \"\"\n",
    "        scratchpad_text = \"\"\n",
    "        in_scratchpad = False\n",
    "\n",
    "        for chunk in stream:\n",
    "            delta = chunk.choices[0].delta\n",
    "            content = delta.content if delta.content else \"\"\n",
    "            if not content:\n",
    "                continue\n",
    "\n",
    "            buffer += content\n",
    "\n",
    "            if \"@@@END_OF_ASSISTANT@@@\" in buffer:\n",
    "                # Split and mark the transition\n",
    "                before, after = buffer.split(\"@@@END_OF_ASSISTANT@@@\", 1)\n",
    "                assistant_text += before\n",
    "                print(before, end=\"\", flush=True)\n",
    "                in_scratchpad = True\n",
    "                scratchpad_text += after\n",
    "                buffer = \"\"\n",
    "                continue\n",
    "\n",
    "            if in_scratchpad:\n",
    "                scratchpad_text += content\n",
    "            else:\n",
    "                assistant_text += content\n",
    "                print(content, end=\"\", flush=True)\n",
    "\n",
    "        print()  # newline\n",
    "\n",
    "        # Store in history and scratchpad\n",
    "        self.conversation_history.append({\"role\": \"user\", \"content\": user_input})\n",
    "        self.conversation_history.append({\"role\": \"assistant\", \"content\": assistant_text.strip()})\n",
    "\n",
    "        try:\n",
    "            match = re.search(r'scratchpad_update:\\s*(.*)', scratchpad_text)\n",
    "            if match:\n",
    "                self.scratchpad.append(match.group(1).strip())\n",
    "        except Exception as e:\n",
    "            print(\"[!] Could not extract scratchpad_update:\", e)\n",
    "\n",
    "        \n",
    "    def run(self):\n",
    "        inp = input(\" \")\n",
    "        while inp != \"quit\":\n",
    "            self.query_gpt4_streaming(inp)\n",
    "            inp = input(\"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "e217071f-2ad0-4cfd-ab2a-2f4152716f50",
   "metadata": {},
   "outputs": [],
   "source": [
    "convo = Conversation()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "0539c982-b40b-4bb0-b64b-f93b10d14c13",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "  Quiero discutir la vida de Fyodor Dostoevsky\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "¬°Claro! Fyodor Dostoevsky es un famoso escritor ruso. ¬øPor d√≥nde te gustar√≠a empezar? ¬øExisten aspectos espec√≠ficos de su vida que te interesan m√°s? ¬øO tal vez prefieres hablar de su obra literaria?@@@END_OF_ASSISTANT¬°Claro! Fyodor Dostoevsky es un famoso escritor ruso. ¬øPor d√≥nde te gustar√≠a empezar? ¬øExisten aspectos espec√≠ficos de su vida que te interesan m√°s? ¬øO tal vez prefieres hablar de su obra literaria?\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "\n",
      " ¬øEn qui√©n a√±o naci√≥ Dostoevsky?\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fyodor Dostoevsky naci√≥ en el a√±o 1821. ¬øTe gustar√≠a saber algo m√°s sobre su vida o su obra?@@@END_OF_ASSISTANT@@Fyodor Dostoevsky naci√≥ en el a√±o 1821. ¬øTe gustar√≠a saber algo m√°s sobre su vida o su obra?\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "\n",
      " quit\n"
     ]
    }
   ],
   "source": [
    "convo.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "8423500f-8430-4d5f-9c71-2aa9f598dc2d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['[No grammar or spelling mistakes.]',\n",
       " 'The user wrote \"¬øEn qui√©n a√±o naci√≥ Dostoevsky?\" instead of \"¬øEn qu√© a√±o naci√≥ Dostoevsky?\". The user used \"qui√©n\" (who) instead of \"qu√©\" (what).']"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "convo.scratchpad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "0b93c902-9857-4f8d-9b9b-6dfcbce7e2c1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'role': 'user', 'content': 'Quiero discutir la vida de Fyodor Dostoevsky.'},\n",
       " {'role': 'assistant', 'content': ''}]"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "convo.conversation_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "28166f08-037c-435c-9efa-00dfc4ca25a1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'role': 'user', 'content': 'Quiero hablar sobre el libro Don Quixote.'},\n",
       " {'role': 'assistant',\n",
       "  'content': '{\"assistant\": \"¬°Perfecto! Don Quijote es una obra maestra de la literatura espa√±ola. ¬øQu√© aspecto de la novela te gustar√≠a discutir?\", \"scratchpad_update\": \"\"}'},\n",
       " {'role': 'user', 'content': 'En que a√±o fue escrito'},\n",
       " {'role': 'assistant',\n",
       "  'content': '{\"assistant\": \"Don Quijote de la Mancha fue escrito por Miguel de Cervantes Saavedra. La primera parte fue publicada en el a√±o 1605 y la segunda parte en 1615.\", \"scratchpad_update\": \"\"}'},\n",
       " {'role': 'user',\n",
       "  'content': '¬øPor qu√© es un libro tan importante en la literatura hispanohablante? No he leer el libro.'},\n",
       " {'role': 'assistant',\n",
       "  'content': '{\"assistant\": \"Don Quijote es considerado un libro importante en la literatura hispanohablante porque es una de las obras m√°s destacadas de la literatura espa√±ola y se considera una de las mejores obras literarias jam√°s escritas. Es una cr√≠tica y una parodia de las novelas de caballer√≠as que estaban de moda en la √©poca de Cervantes. A trav√©s de las aventuras de Don Quijote y Sancho Panza, Cervantes reflexiona sobre la realidad y la ficci√≥n, la cordura y la locura, el bien y el mal, entre otros temas. Aunque no lo hayas le√≠do, te recomiendo hacerlo.\", \"scratchpad_update\": \"Incorrect verb conjugation. The correct form should be \\'No he le√≠do el libro.\\' not \\'No he leer el libro.\\'\"}'}]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "convo.conversation_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "8b99d131-b2b0-44e8-a6c2-af63c982c7e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A m√≠ tambi√©n me gustan las bibliotecas. ¬øCu√°l es tu libro favorito?\n"
     ]
    }
   ],
   "source": [
    "convo.query_gpt4(\"A mi me gusta las bibliotecas\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "c7574a76-87fe-475d-b639-9d363aaea0c0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'role': 'user', 'content': 'A mi me gusta las bibliotecas'},\n",
       " {'role': 'assistant',\n",
       "  'content': 'A m√≠ tambi√©n me gustan las bibliotecas. ¬øCu√°l es tu libro favorito?'}]"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "convo.conversation_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "7adeb3a5-3f10-4bb2-b9a9-6a7bdded29e4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'mistake': 'A mi me gusta las bibliotecas',\n",
       "  'correction': 'A m√≠ me gustan las bibliotecas',\n",
       "  'explanation': \"The verb 'gustar' must agree with the plural noun 'bibliotecas', so it should be 'gustan'. Also, there is an accent on 'm√≠' to distinguish it from the possessive 'mi'.\"}]"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "convo.scratchpad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "f2c3cad4-412a-4302-86cd-ad99fc03c4d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "run = client.beta.threads.runs.retrieve(thread_id=convo.thread.id, run_id=\"run_hhoObNG2Flj5Enzr27OL2wi4\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "8533a37f-8b73-45b4-afd3-6e0aec480273",
   "metadata": {},
   "outputs": [],
   "source": [
    "tool_calls = run.required_action.submit_tool_outputs.tool_calls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "05e721b4-8fcd-481c-b293-13226ef83cf9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[RequiredActionFunctionToolCall(id='call_Nn5zKoSlXzo01umEnbTbrBGK', function=Function(arguments='{\"mistake\":\"A mi me gusta las bibliotecas\",\"correction\":\"A m√≠ me gustan las bibliotecas\",\"explanation\":\"The correct form is \\'A m√≠ me gustan\\' because the verb \\'gustar\\' should agree with the plural noun \\'bibliotecas\\'.\"}', name='log_mistake_to_scratchpad'), type='function')]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tool_calls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "835025ec-be3e-45e5-9926-ea272ccac3c0",
   "metadata": {},
   "outputs": [
    {
     "ename": "BadRequestError",
     "evalue": "Error code: 400 - {'error': {'message': 'Thread thread_omsHoO1Oe9ZhewkGHO22WgY8 already has an active run run_sXPV9WLvucw6zK4Af4sBObKV.', 'type': 'invalid_request_error', 'param': None, 'code': None}}",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mBadRequestError\u001b[0m                           Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_1837830/916596949.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m run = client.beta.threads.runs.create_and_poll(\n\u001b[0m\u001b[1;32m      2\u001b[0m                 \u001b[0mthread_id\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconvo\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mthread\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mid\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m                 \u001b[0massistant_id\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconvo\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0massistant\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mid\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m         )\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.10/site-packages/openai/resources/beta/threads/runs/runs.py\u001b[0m in \u001b[0;36mcreate_and_poll\u001b[0;34m(self, assistant_id, include, additional_instructions, additional_messages, instructions, max_completion_tokens, max_prompt_tokens, metadata, model, parallel_tool_calls, response_format, temperature, tool_choice, tools, top_p, truncation_strategy, poll_interval_ms, thread_id, extra_headers, extra_query, extra_body, timeout)\u001b[0m\n\u001b[1;32m    786\u001b[0m         \u001b[0mhttps\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m//\u001b[0m\u001b[0mplatform\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopenai\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcom\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mdocs\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0massistants\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mhow\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mit\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mworks\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mruns\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;32mand\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0msteps\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    787\u001b[0m         \"\"\"\n\u001b[0;32m--> 788\u001b[0;31m         run = self.create(\n\u001b[0m\u001b[1;32m    789\u001b[0m             \u001b[0mthread_id\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mthread_id\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    790\u001b[0m             \u001b[0massistant_id\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0massistant_id\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.10/site-packages/openai/_utils/_utils.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    273\u001b[0m                         \u001b[0mmsg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34mf\"Missing required argument: {quote(missing[0])}\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    274\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 275\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    276\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    277\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m  \u001b[0;31m# type: ignore\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.10/site-packages/openai/resources/beta/threads/runs/runs.py\u001b[0m in \u001b[0;36mcreate\u001b[0;34m(self, thread_id, assistant_id, include, additional_instructions, additional_messages, instructions, max_completion_tokens, max_prompt_tokens, metadata, model, parallel_tool_calls, response_format, stream, temperature, tool_choice, tools, top_p, truncation_strategy, extra_headers, extra_query, extra_body, timeout)\u001b[0m\n\u001b[1;32m    533\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Expected a non-empty value for `thread_id` but received {thread_id!r}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    534\u001b[0m         \u001b[0mextra_headers\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m\"OpenAI-Beta\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m\"assistants=v2\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mextra_headers\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 535\u001b[0;31m         return self._post(\n\u001b[0m\u001b[1;32m    536\u001b[0m             \u001b[0;34mf\"/threads/{thread_id}/runs\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    537\u001b[0m             body=maybe_transform(\n",
      "\u001b[0;32m~/.local/lib/python3.10/site-packages/openai/_base_client.py\u001b[0m in \u001b[0;36mpost\u001b[0;34m(self, path, cast_to, body, options, files, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1276\u001b[0m             \u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"post\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mjson_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbody\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfiles\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mto_httpx_files\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiles\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1277\u001b[0m         )\n\u001b[0;32m-> 1278\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mResponseT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcast_to\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mopts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstream\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstream\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstream_cls\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstream_cls\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1279\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1280\u001b[0m     def patch(\n",
      "\u001b[0;32m~/.local/lib/python3.10/site-packages/openai/_base_client.py\u001b[0m in \u001b[0;36mrequest\u001b[0;34m(self, cast_to, options, remaining_retries, stream, stream_cls)\u001b[0m\n\u001b[1;32m    953\u001b[0m             \u001b[0mretries_taken\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    954\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 955\u001b[0;31m         return self._request(\n\u001b[0m\u001b[1;32m    956\u001b[0m             \u001b[0mcast_to\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcast_to\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    957\u001b[0m             \u001b[0moptions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.10/site-packages/openai/_base_client.py\u001b[0m in \u001b[0;36m_request\u001b[0;34m(self, cast_to, options, retries_taken, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1057\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1058\u001b[0m             \u001b[0mlog\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdebug\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Re-raising status error\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1059\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_status_error_from_response\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1060\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1061\u001b[0m         return self._process_response(\n",
      "\u001b[0;31mBadRequestError\u001b[0m: Error code: 400 - {'error': {'message': 'Thread thread_omsHoO1Oe9ZhewkGHO22WgY8 already has an active run run_sXPV9WLvucw6zK4Af4sBObKV.', 'type': 'invalid_request_error', 'param': None, 'code': None}}"
     ]
    }
   ],
   "source": [
    "run = client.beta.threads.runs.create_and_poll(\n",
    "                thread_id=convo.thread.id,\n",
    "                assistant_id=convo.assistant.id\n",
    "        )\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "866d2708-1b5a-4584-951c-037af7a48526",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "8d2f8e81-059c-46a2-9599-be6d85a987b7",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting SpeechRecognition\n",
      "  Using cached speechrecognition-3.14.3-py3-none-any.whl (32.9 MB)\n",
      "Collecting pyaudio\n",
      "  Using cached PyAudio-0.2.14.tar.gz (47 kB)\n",
      "  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
      "\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting playsound\n",
      "  Using cached playsound-1.3.0-py3-none-any.whl\n",
      "Requirement already satisfied: typing-extensions in /home/users/sv226/.local/lib/python3.10/site-packages (from SpeechRecognition) (4.14.0)\n",
      "Building wheels for collected packages: pyaudio\n",
      "  Building wheel for pyaudio (pyproject.toml) ... \u001b[?25lerror\n",
      "  \u001b[1;31merror\u001b[0m: \u001b[1msubprocess-exited-with-error\u001b[0m\n",
      "  \n",
      "  \u001b[31m√ó\u001b[0m \u001b[32mBuilding wheel for pyaudio \u001b[0m\u001b[1;32m(\u001b[0m\u001b[32mpyproject.toml\u001b[0m\u001b[1;32m)\u001b[0m did not run successfully.\n",
      "  \u001b[31m‚îÇ\u001b[0m exit code: \u001b[1;36m1\u001b[0m\n",
      "  \u001b[31m‚ï∞‚îÄ>\u001b[0m \u001b[31m[17 lines of output]\u001b[0m\n",
      "  \u001b[31m   \u001b[0m running bdist_wheel\n",
      "  \u001b[31m   \u001b[0m running build\n",
      "  \u001b[31m   \u001b[0m running build_py\n",
      "  \u001b[31m   \u001b[0m creating build\n",
      "  \u001b[31m   \u001b[0m creating build/lib.linux-x86_64-3.10\n",
      "  \u001b[31m   \u001b[0m creating build/lib.linux-x86_64-3.10/pyaudio\n",
      "  \u001b[31m   \u001b[0m copying src/pyaudio/__init__.py -> build/lib.linux-x86_64-3.10/pyaudio\n",
      "  \u001b[31m   \u001b[0m running build_ext\n",
      "  \u001b[31m   \u001b[0m creating build/temp.linux-x86_64-3.10\n",
      "  \u001b[31m   \u001b[0m creating build/temp.linux-x86_64-3.10/src\n",
      "  \u001b[31m   \u001b[0m creating build/temp.linux-x86_64-3.10/src/pyaudio\n",
      "  \u001b[31m   \u001b[0m x86_64-linux-gnu-gcc -Wno-unused-result -Wsign-compare -DNDEBUG -g -fwrapv -O2 -Wall -g -fstack-protector-strong -Wformat -Werror=format-security -g -fwrapv -O2 -fPIC -I/usr/local/include -I/usr/include -I/usr/include/python3.10 -c src/pyaudio/device_api.c -o build/temp.linux-x86_64-3.10/src/pyaudio/device_api.o\n",
      "  \u001b[31m   \u001b[0m src/pyaudio/device_api.c:9:10: fatal error: portaudio.h: No such file or directory\n",
      "  \u001b[31m   \u001b[0m     9 | #include \"portaudio.h\"\n",
      "  \u001b[31m   \u001b[0m       |          ^~~~~~~~~~~~~\n",
      "  \u001b[31m   \u001b[0m compilation terminated.\n",
      "  \u001b[31m   \u001b[0m error: command '/usr/bin/x86_64-linux-gnu-gcc' failed with exit code 1\n",
      "  \u001b[31m   \u001b[0m \u001b[31m[end of output]\u001b[0m\n",
      "  \n",
      "  \u001b[1;35mnote\u001b[0m: This error originates from a subprocess, and is likely not a problem with pip.\n",
      "\u001b[31m  ERROR: Failed building wheel for pyaudio\u001b[0m\u001b[31m\n",
      "\u001b[0m\u001b[?25hFailed to build pyaudio\n",
      "\u001b[31mERROR: Could not build wheels for pyaudio, which is required to install pyproject.toml-based projects\u001b[0m\u001b[31m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip3 install SpeechRecognition pyaudio playsound"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f931384-3294-4f95-9a61-dbb5f7a55090",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "9b8b10dd-735f-42a1-b194-fd9422bf8537",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'speech_recognition'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_1869291/4028562010.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mopenai\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mspeech_recognition\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0msr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtempfile\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mplaysound\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'speech_recognition'"
     ]
    }
   ],
   "source": [
    "import openai\n",
    "import speech_recognition as sr\n",
    "import tempfile\n",
    "import os\n",
    "import playsound\n",
    "\n",
    "openai.api_key = api_key  # replace with your actual key\n",
    "\n",
    "# üé§ Step 1: Record audio and transcribe using Whisper\n",
    "def transcribe_with_whisper():\n",
    "    recognizer = sr.Recognizer()\n",
    "    with sr.Microphone() as source:\n",
    "        print(\"üéôÔ∏è Speak now...\")\n",
    "        audio = recognizer.listen(source)\n",
    "\n",
    "    with tempfile.NamedTemporaryFile(suffix=\".wav\", delete=False) as tmpfile:\n",
    "        wav_path = tmpfile.name\n",
    "        with open(wav_path, \"wb\") as f:\n",
    "            f.write(audio.get_wav_data())\n",
    "\n",
    "    with open(wav_path, \"rb\") as f:\n",
    "        transcription = openai.audio.transcriptions.create(\n",
    "            model=\"whisper-1\",\n",
    "            file=f\n",
    "        )\n",
    "    os.remove(wav_path)\n",
    "    return transcription.text\n",
    "\n",
    "# üß† Step 2: Use GPT-4 to respond (optional)\n",
    "def gpt_response(prompt, system_instruction=\"You are a helpful assistant.\"):\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": system_instruction},\n",
    "        {\"role\": \"user\", \"content\": prompt}\n",
    "    ]\n",
    "    response = openai.chat.completions.create(\n",
    "        model=\"gpt-4\",\n",
    "        messages=messages\n",
    "    )\n",
    "    return response.choices[0].message.content\n",
    "\n",
    "# üîä Step 3: Convert GPT reply to speech using OpenAI TTS\n",
    "def speak_with_tts(text, voice=\"nova\"):\n",
    "    response = openai.audio.speech.create(\n",
    "        model=\"tts-1\",\n",
    "        voice=voice,\n",
    "        input=text\n",
    "    )\n",
    "    with tempfile.NamedTemporaryFile(suffix=\".mp3\", delete=False) as tmpfile:\n",
    "        tmpfile.write(response.content)\n",
    "        mp3_path = tmpfile.name\n",
    "\n",
    "    playsound.playsound(mp3_path)\n",
    "    os.remove(mp3_path)\n",
    "\n",
    "# üîÅ Run a single loop\n",
    "if __name__ == \"__main__\":\n",
    "    try:\n",
    "        user_text = transcribe_with_whisper()\n",
    "        print(\"You said:\", user_text)\n",
    "\n",
    "        reply = gpt_response(user_text)\n",
    "        print(\"GPT says:\", reply)\n",
    "\n",
    "        speak_with_tts(reply)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(\"[!] Error:\", e)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd41abac-d295-4107-997b-ddb753fec289",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
